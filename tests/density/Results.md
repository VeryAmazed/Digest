I wanted to do some checks on the behavior of my program and so I decided to throw together some software to look at the density of minimizers, $\frac{number-of-minimizers}{number-of-total-kmers}$. <br>
# Set Up
For this series of experiments, the small window size was set to 16 for everything, and the sequence length was set to $10^5$. I wrote a program to randomly generate 100 sequences that only contained ACTG characters and another 100 sequences that could contain the character N, along with ACTG characters. So for the ACTG only sequences, there are 99985 kmers, and for the non-ACTG allowed sequences, the probability of a character being N was set to 1/33, and on average the number of kmers was between 60000 and 62000. The program I wrote does an actual count of how many k-mers are in the sequence following the same skipping rules that the Digester does. <br>

For the Mod Minimizer, I ran tests using 4 different mod values, M, 2 prime and 2 not prime. I thought of X as a Bernoulli(1/M) representing the probability that a given kmer in the sequence was a minimizer. <br>

For the [Window Minimizer](https://academic.oup.com/bioinformatics/article/33/14/i110/3953951) and [Syncmer](https://peerj.com/articles/10805/), apparently the expected density of minimizers is $\frac{2}{w+1}$ and $\frac{2}{w}$ respectively, where w is the number of kmers in the large window. The syncmers I implement are what the paper calls closed syncmers. The explanation for these densities, briefly and assuming hashes are i.i.d, is that for window minimizers is that when you slide the large window over 1 spot, you get a new minimzer if the the hash of the kmer that just left the window was minimal, or if the hash for the newly introduced kmer is minimal, giving $P_{m} = \frac{2}{w+1}$. For syncmers, the probability a large window is a syncmer is if either it's left or rightmost kmer have minimal hashes giving $P_{s} = \frac{2}{w}$. Viewing these as both being Bernoulli r.v.s then $E[x] = P_m$ and $E[x] = P_s$. Although the way I implemented Syncmers is a bit different from what is described in the paper because I don't break ties for Syncmers, I just check to make sure that the smallest hash value in the large window is also equal to either the hash value of the leftmost or rightmost kmer. But, since w is set to 16, and ntHash is a 64 bit hash, the probability of ties is basically zero. <br>
# Results
The graphs for the set of tests run on the ACTG only sequences are in the ACTG_Only_Graphs folder and the graphs for the set of tests run on sequences that contained N are in the Not_ACTG_Only_Graphs folder. <br>

All graphs look normal and all 3 methods of obtaining minizmers are normal about the theoretical expected value.<br>

I am not sure if the Central Limit Theorem can apply here. I think with how things are modeled, samples are drawn with replacement, but also universal hashes are only pairwise independent as opposed to completely independent, and furthermore, Window Minimizers and Syncmers most certainly are not independent as whether the current kmer is a minimizer or the smallest in the window is very much affected by what the previous minimizer was and what other values in the window are. However, I imagine they do satisfy the condition for being considered weakly dependent as if two kmers are significantly far apart, the first kmer will have no overlap with the second kmer and thus their hash values will tell you nothing about one another, additonally for Window Minimizers and Syncmers, if large windows are sufficiently far apart then know what the minimizer was for one large window tells you nothing about the other. <br>
